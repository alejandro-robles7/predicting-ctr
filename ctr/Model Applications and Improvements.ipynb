{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Applications and Improvements\n",
    "\n",
    "It’s time to dive deeper. Find out how you can use measures of model performance including precision and recall to answer real-world questions, such as evaluating ROI on ad spend. You’ll also learn ways to improve upon those evaluation metrics, such as ensemble methods and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating four categories\n",
    "The confusion matrix is the most straightforward tool to look at the four categories of outcomes: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). In this exercise, you will use a standard decision tree classifier DecisionTreeClassifier() from sklearn on the sample click data and calculate the breakdowns of outcomes by the four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_pickle\n",
    "\n",
    "df = read_pickle('data/data_ch3.pkl')\n",
    "# # Get non-categorical columns, with a filter\n",
    "num_df = df.select_dtypes(include=['int', 'float'])\n",
    "filter_cols = ['banner_pos', 'hour_of_day']\n",
    "new_df = num_df[num_df.columns[~num_df.columns.isin(filter_cols)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: 86, FP: 3, FN: 9, TP: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "X = new_df.loc[:, ~new_df.columns.isin(['click'])]\n",
    "y = new_df.click\n",
    "\n",
    "# Set up classifier using training data to predict test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size = .2, random_state = 0)\n",
    "clf = DecisionTreeClassifier()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "\n",
    "# Define confusion matrix and four categories\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn = conf_matrix[0][0]\n",
    "fp = conf_matrix[0][1]\n",
    "fn = conf_matrix[1][0]\n",
    "tp = conf_matrix[1][1]\n",
    "\n",
    "print(\"TN: %s, FP: %s, FN: %s, TP: %s\" %(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the largest category is TN (model predicts no click, and actual was no click), which makes sense because our model will mostly predict non-click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI on ad spend\n",
    "The return on investment (ROI) for ad spend can be categorized using the four outcomes from a confusion matrix. This quantity is defined as the ratio between the total return and the total cost. If this quantity is greater than 1, it indicates the total return was greater than the total cost and vice versa. In this exercise, you will compute a sample ROI assuming a fixed r, the return on a click per number of impressions, and cost, the cost per number of impressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total return: 0.4, Total cost: 0.30000000000000004, ROI: 1.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix and get four categories\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate total return, total spent, and ROI\n",
    "r = 0.2\n",
    "cost = 0.05\n",
    "total_return = tp * r\n",
    "total_cost = (tp + fp) * cost \n",
    "roi = total_return / total_cost\n",
    "print(\"Total return: %s, Total cost: %s, ROI: %s\" %(\n",
    "  total_return, total_cost, roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the ROI was > 1 and hence the total return was more than the total cost. Since the total return exceeds the cost, such an ad campaign is worth it for a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall\n",
    "Both precision and recall are related to the four outcomes discussed in the prior lesson and are important evaluation metrics for any machine learning model. An ad CTR model should ideally have high precision (high ROI on ad spend) and recall (relevant audience targeting). Although it is possible to calculate precision and recall by hand, sklearn has some handy implementations that you can easily plug into the existing workflow. In this exercise, you will set up a decision tree and calculate precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8496842105263158, Recall: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Set up training and testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size = .2, random_state = 0)\n",
    "\n",
    "# Create classifier and make predictions\n",
    "clf = DecisionTreeClassifier()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "\n",
    "# Evaluate precision and recall\n",
    "prec = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Precision: %s, Recall: %s\" %(prec, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision value is the proportion of clicks relative to total number of impressions, and recall is the proportion of clicks gotten of all clicks available. Both are around 80-85%! Now let's jump into comparisons with other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Evaluating a classifier relative to an appropriate baseline is important. This is especially true for imbalanced datasets, such as ad click-through, because high accuracy can easily be achieved through always selecting the majority class. In this exercise, you will simulate a baseline classifier that always predicts the majority class (non-click) and look at its confusion matrix, as well as what its precision and recall are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "[[89  0]\n",
      " [11  0]]\n",
      "Precision: 0.7921, Recall: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.robles/Library/Python/3.7/lib/python/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# Set up baseline predictions\n",
    "y_pred = asarray([0 for x in range(len(X_test))])\n",
    "\n",
    "# Look at confusion matrix\n",
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Check precision and recall\n",
    "prec = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Precision: %s, Recall: %s\" %(prec, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of true and false positives are both 0, which is expected by design. Also note that the recall here was 83% simply due to the imbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier comparison\n",
    "The ROI framework can be run across different classifiers to see how higher precision and recall lead to higher ROI values. Note that the baseline classifier you created would have a total return and cost of 0 since both the true positives tp and false positives fp will be 0 by design. In this exercise, you will use the ROI framework to compare a logistic regression and decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total return: 0.0, Total spent: 0.0, ROI: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.robles/Library/Python/3.7/lib/python/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Create and fit classifier\n",
    "clf = LogisticRegression()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "\n",
    "# Calculate total return, total spent, and ROI \n",
    "r, cost = 0.2, 0.05\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "total_return = tp * r\n",
    "total_spent = (tp + fp) * cost \n",
    "roi = total_return / total_spent\n",
    "print(\"Total return: %s, Total spent: %s, ROI: %s\" %(total_return, total_spent, roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total return: 0.4, Total spent: 0.25, ROI: 1.6\n"
     ]
    }
   ],
   "source": [
    "# Create and fit decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "\n",
    "# Calculate total return, total spent, and ROI \n",
    "r, cost = 0.2, 0.05\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "total_return = tp * r\n",
    "total_spent = (tp + fp) * cost \n",
    "roi = total_return / total_spent\n",
    "print(\"Total return: %s, Total spent: %s, ROI: %s\" %(total_return, total_spent, roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the logistic regression classifier had total return of 0 and total spend of 0 - this is because it predicted only 0's, similar to the baseline classifier. Using a decision tree classifier, we see that the ROI > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Regularization is the process of adding information to a model in order to prevent overfitting. This is important in order to boost the evaluation metrics you saw earlier in the chapter. In this exercise, you will vary around the max depth parameter of a decision tree in order to see how the classification results are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating tree with max_depth = 2\n",
      "Confusion matrix: \n",
      "[[89  0]\n",
      " [11  0]]\n",
      "Precision: 0.7921, Recall: 0.89\n",
      "Evaluating tree with max_depth = 3\n",
      "Confusion matrix: \n",
      "[[85  4]\n",
      " [ 9  2]]\n",
      "Precision: 0.8414539007092199, Recall: 0.87\n",
      "Evaluating tree with max_depth = 5\n",
      "Confusion matrix: \n",
      "[[84  5]\n",
      " [ 9  2]]\n",
      "Precision: 0.8352995391705069, Recall: 0.86\n",
      "Evaluating tree with max_depth = 10\n",
      "Confusion matrix: \n",
      "[[86  3]\n",
      " [ 9  2]]\n",
      "Precision: 0.8496842105263158, Recall: 0.88\n",
      "Evaluating tree with max_depth = 15\n",
      "Confusion matrix: \n",
      "[[85  4]\n",
      " [ 9  2]]\n",
      "Precision: 0.8414539007092199, Recall: 0.87\n",
      "Evaluating tree with max_depth = 20\n",
      "Confusion matrix: \n",
      "[[85  4]\n",
      " [ 9  2]]\n",
      "Precision: 0.8414539007092199, Recall: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.robles/Library/Python/3.7/lib/python/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different levels of max depth\n",
    "for max_depth_val in [2, 3, 5, 10, 15, 20]:\n",
    "  # Create and fit model\n",
    "  clf = DecisionTreeClassifier(max_depth = max_depth_val)\n",
    "  print(\"Evaluating tree with max_depth = %s\" %(max_depth_val))\n",
    "  y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "  \n",
    "  # Evaluate confusion matrix, precision, recall\n",
    "  print(\"Confusion matrix: \")\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  prec = precision_score(y_test, y_pred, average = 'weighted')\n",
    "  recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "  print(\"Precision: %s, Recall: %s\" %(prec, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the the recall levels are fairly close to one another, and a max depth of 10 levels has the highest precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "Cross validation is a technique that attempts to check on a model's holdout performance. It is done to ensure that the testing performance was not due to any particular issues on splitting of data. In this exercise, you will use implementations from sklearn to run a K-fold cross validation by using the KFold() module to assess cross validation to assess precision and recall for a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision scores: [0.8285461  0.7303533  0.74347158 0.73416667]\n",
      "Recall scores: [0.85 0.79 0.8  0.81]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Create model \n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Set up k-fold\n",
    "k_fold = KFold(n_splits = 4, random_state = 0)\n",
    "\n",
    "# Evaluate precision and recall for each fold\n",
    "precision = cross_val_score(\n",
    "  clf, X_train, y_train, cv = k_fold, scoring = 'precision_weighted')\n",
    "recall = cross_val_score(\n",
    "  clf, X_train, y_train, cv = k_fold, scoring = 'recall_weighted')\n",
    "print(\"Precision scores: %s\" %(precision)) \n",
    "print(\"Recall scores: %s\" %(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall levels are fairly close to one another - showing that the testing split did not affect the classifier's results. In real-life settings, the practical takeaway is that using cross validation is very powerful for large datasets since you want to avoid overfitting and different splits will likely give differen results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Both regularization and cross validation are powerful tools in model selection. Regularization can help prevent overfitting and cross validation ensures that your models are being evaluated properly. In this exercise, you will use regularization and cross validation together and see whether or not models differ significantly. You will calculate the precision only, although the same exercise can easily be done for recall and other evaluation metrics as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree for max_depth = 3\n",
      "Cross validation Precision: [0.7794898  0.70926316 0.71989796 0.6889    ]\n",
      "Test Precision: 0.8414539007092199\n",
      "Evaluating Decision Tree for max_depth = 5\n",
      "Cross validation Precision: [0.7794898  0.66625    0.75473684 0.75421986]\n",
      "Test Precision: 0.8304347826086956\n",
      "Evaluating Decision Tree for max_depth = 10\n",
      "Cross validation Precision: [0.81112135 0.7303533  0.74840426 0.73416667]\n",
      "Test Precision: 0.8496842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.robles/Library/Python/3.7/lib/python/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different levels of max depth and set up k-fold\n",
    "for max_depth_val in [3, 5, 10]:\n",
    "  k_fold = KFold(n_splits = 4, random_state = 0)\n",
    "  clf = DecisionTreeClassifier(max_depth = max_depth_val)\n",
    "  print(\"Evaluating Decision Tree for max_depth = %s\" %(max_depth_val))\n",
    "  y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "  \n",
    "  # Calculate precision for cross validation and test\n",
    "  cv_precision = cross_val_score(\n",
    "    clf, X_train, y_train, cv = k_fold, scoring = 'precision_weighted')\n",
    "  precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "  print(\"Cross validation Precision: %s\" %(cv_precision))\n",
    "  print(\"Test Precision: %s\" %(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the the recall levels are fairly close to one another, and a max depth of 10 has the highest precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "Random Forests are a classic and powerful ensemble method that utilize individual decision trees via bootstrap aggregation (or bagging for short). Two main hyperparameters involved in this type of model are the number of trees, and the max depth of each tree. In this exercise, you will implement and evaluate a simple random forest classifier with some fixed hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC of AUC: 0.5960163432073545\n",
      "Precision: 0.8496842105263158, Recall: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create random forest classifier with specified params\n",
    "clf = RandomForestClassifier(n_estimators= 50, max_depth = 5)\n",
    "\n",
    "# Train classifier - predict probability score and label\n",
    "y_score = clf.fit(X_train, y_train).predict_proba(X_test) \n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test) \n",
    "\n",
    "# Get ROC curve metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\n",
    "print(\"ROC of AUC: %s\"%(auc(fpr, tpr)))\n",
    "\n",
    "# Get precision and recall\n",
    "precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Precision: %s, Recall: %s\" %(precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that is has better performance than logistic regression but is fairly close to that of decision trees. This is likely due to the model being not very complex (since we did not set it up that way). Now let's explore tuning the various hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "Hyperparameter tuning can be done by sklearn through providing various input parameters, each of which can be encoded using various functions from numpy. One method of tuning, which exhaustively looks at all combinations of input hyperparameters specified via param_grid, is grid search. In this exercise, you will use grid search to look over the hyperparameters for a sample random forest classifier with a scoring function as the AUC of the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting RF grid search.. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.robles/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: \n",
      "0.6237490875262615\n",
      "Best Estimator: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create list of hyperparameters \n",
    "n_estimators = [10, 50]\n",
    "max_depth = [5, 20]\n",
    "param_grid = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "\n",
    "# Use Grid search CV to find best parameters \n",
    "print(\"starting RF grid search.. \")\n",
    "rf = RandomForestClassifier()\n",
    "clf = GridSearchCV(estimator = rf, param_grid = param_grid, scoring = 'roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best Score: \")\n",
    "print(clf.best_score_)\n",
    "print(\"Best Estimator: \")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen here, the best AUC is around 0.7 and from the model with the maximum number of trees (50) and maximum depth (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
